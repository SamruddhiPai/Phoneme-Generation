{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw3p2_bootcamp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqxhejj_yxxo"
      },
      "source": [
        "# 1 Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPhXLOOGy23O"
      },
      "source": [
        "## 1.1 Google Drive - Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e15Pr4YZyzx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda9b51e-96a3-4f58-9e66-89e6dabebc7e"
      },
      "source": [
        "# Google drive setup\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGghymIKyzkY",
        "outputId": "80124f98-d3c8-4a07-9be4-fb640dcc379d"
      },
      "source": [
        "#Intall Kaggle API and create kaggle directory\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!mkdir .kaggle\n",
        "#This data is used to login  into your Kaggle account\n",
        "import json\n",
        "token = {\"username\":\"samruddhi98\",\"key\":\"db26269bc2e5ae4d4f8456490e50b5a8\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 35.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 4.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=c80cd86342cee1ee624ba4b2109b887ade0a731a1dcd8b135a3a647b5189e328\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChpO-wzqgDT9",
        "outputId": "a8b18e00-44b9-4048-e9ed-c11fa97c7f98"
      },
      "source": [
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "\n",
        "!cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "!kaggle config set -n path -v /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- path is now set to: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDAvSWLFzHTy"
      },
      "source": [
        "## 1.2 Kaggle Data Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1wzMmsQSjO6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhUzmdN9BMha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af523f01-0495-4874-f62a-a470d4fb524d"
      },
      "source": [
        "# download data\n",
        "!kaggle competitions download -c 11785-fall2021-hw3p2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 11785-fall2021-hw3p2.zip to /content/competitions/11785-fall2021-hw3p2\n",
            "100% 2.34G/2.35G [00:13<00:00, 157MB/s]\n",
            "100% 2.35G/2.35G [00:13<00:00, 191MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lxbGtNvzP5x"
      },
      "source": [
        "!mkdir data\n",
        "\n",
        "!unzip -qo '/content/competitions/11785-fall2021-hw3p2/11785-fall2021-hw3p2.zip' -d data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v4SIZkWz8xo"
      },
      "source": [
        "# !ls data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWjk6wdDKGM0"
      },
      "source": [
        "## 1.3 Library Installations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCW-7s2CKRE6"
      },
      "source": [
        "Install [ctcdecode](https://github.com/parlance/ctcdecode)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWnh9OW2KFC6"
      },
      "source": [
        "# !git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "# !pip install wget\n",
        "# %cd ctcdecode\n",
        "# !pip install .\n",
        "# %cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UGGXQpUgaob",
        "outputId": "b8a58318-7415-4706-f200-3c3c9e4d93c3"
      },
      "source": [
        "cd /content/gdrive/MyDrive/IDL-HW3P2/ctcdecode/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/IDL-HW3P2/ctcdecode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8YL5tapgcBt",
        "outputId": "9ba9209a-3d4f-4021-a1ae-e677d7fc6e80"
      },
      "source": [
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/gdrive/MyDrive/IDL-HW3P2/ctcdecode\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13266924 sha256=c3e815322c36605ad20c9606ed6ac169bc343533a8e6c5c8c31089299c0c9afe\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gjvx5719/wheels/dc/79/7e/b9a48e94d5318a2ee0954396284f99f962100cefa892bbbd99\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkSTMjLzgcgh",
        "outputId": "6807a874-4bd9-4a1a-a8a8-3b321a609487"
      },
      "source": [
        "cd ../../../../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAI9HBZ7gcyq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFq2DZ1JKmx0"
      },
      "source": [
        "Install [levenshtein distance calculation library](https://github.com/ztane/python-Levenshtein) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wV1_xPiKiJV",
        "outputId": "e7305d0c-fd39-4ca9-a3e2-881bf0a2e970"
      },
      "source": [
        "!pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149864 sha256=7f081412e91d353883e83b5716fb9ccf2be5aae4605b5719d5fdbc4d6fffb67e\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yqlnQRA0DZk"
      },
      "source": [
        "## 1.4 Libraries & Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAHPFFCc0C7b"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import Levenshtein\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pdb\n",
        "import gc\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvSpD3_q0UHH",
        "outputId": "ecc5e02b-02e9-4682-cfbe-565c8b7ba5e2"
      },
      "source": [
        "# Check if cuda is available and set device\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "num_workers = 8 if cuda else 0\n",
        "\n",
        "print(\"Cuda = \", str(cuda), \" with num_workers = \", str(num_workers),  \" system version = \", sys.version)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda =  True  with num_workers =  8  system version =  3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfj-okuK8gM7"
      },
      "source": [
        "configs = {'mode': 'train', 'window_length': 30, \n",
        "           'root_path': '/content/HW3P2_Data/', \n",
        "           'batch_size': 32, 'nhead': 1, 'max_epoch': 200, 'lr': 0.005, \n",
        "           'weight_decay': 5e-05, 'step': 5, 'lr_decay': 0.5, 'gpu': 0, \n",
        "           'ckpt': '/content/gdrive/MyDrive/IDL-HW3P2/saved_model/'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YopAeO_XRhTQ"
      },
      "source": [
        "# 2 Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo7GEsBa4sR"
      },
      "source": [
        "## 2.1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BDZrzzlF5sq"
      },
      "source": [
        "# load training and dev data\n",
        "train_data = np.load('/content/data/HW3P2_Data/train.npy', allow_pickle=True)\n",
        "train_labels = np.load('/content/data/HW3P2_Data/train_labels.npy', allow_pickle=True)\n",
        "\n",
        "dev_data = np.load('/content/data/HW3P2_Data/dev.npy', allow_pickle=True)\n",
        "dev_labels = np.load('/content/data/HW3P2_Data/dev_labels.npy', allow_pickle=True)\n",
        "\n",
        "# load test data\n",
        "test_data = np.load('/content/data/HW3P2_Data/test.npy', allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf89XpGrmYHq",
        "outputId": "adca251c-f0cd-49af-8f39-1d501e0135bc"
      },
      "source": [
        "print(f'Train data: {train_data.shape}')\n",
        "print(f'Train labels {train_labels.shape}')\n",
        "\n",
        "print(f'Dev data: {dev_data.shape}')\n",
        "print(f'Dev labels {dev_labels.shape}')\n",
        "\n",
        "print(f'Test data: {test_data.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: (14542,)\n",
            "Train labels (14542,)\n",
            "Dev data: (2200,)\n",
            "Dev labels (2200,)\n",
            "Test data: (2561,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iLs7aLgbB8Q"
      },
      "source": [
        "## 2.2 Custom Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-0Seqh4Cp9F"
      },
      "source": [
        "# Define dataset class\n",
        "class MyDataSet(Dataset):\n",
        "  # load the dataset\n",
        "  def __init__(self, x, y):\n",
        "    self.X = x\n",
        "    self.Y = y\n",
        "\n",
        "  # get number of items/rows in dataset\n",
        "  def __len__(self):\n",
        "    return len(self.Y)\n",
        "\n",
        "  # get row item at some index\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor(self.X[index])\n",
        "    y = torch.LongTensor(self.Y[index])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def collate_fn(batch):\n",
        "    batch_x = [x for x,y in batch]\n",
        "    batch_y = [y for x,y in batch]\n",
        "\n",
        "    len_x = [len(x) for x,y in batch]\n",
        "    len_y = [len(y) for x,y in batch]\n",
        "\n",
        "    len_x = torch.LongTensor(len_x)\n",
        "    len_y = torch.LongTensor(len_y)\n",
        "\n",
        "    batch_x = pad_sequence(batch_x, batch_first=True)\n",
        "    batch_y = pad_sequence(batch_y, batch_first=True)\n",
        "\n",
        "    return batch_x, batch_y, len_x, len_y\n",
        "    # TODO: Pad sequence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYTDtHEGHsWZ"
      },
      "source": [
        "# Define dataset class\n",
        "class TestDataSet(Dataset):\n",
        "  # load the dataset\n",
        "  # TODO: replace x and y with dataset path and load data from here -> more efficient\n",
        "  def __init__(self, x):\n",
        "    self.X = x\n",
        "\n",
        "  # get number of items/rows in dataset\n",
        "  def __len__(self):\n",
        "    return len(self.X) \n",
        "\n",
        "  # get row item at some index\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor(self.X[index])\n",
        "    return x\n",
        "\n",
        "  def collate_fn(batch):\n",
        "    # TODO: Pad X\n",
        "    batch_x = [x for x in batch]\n",
        "    \n",
        "    len_x = [len(x) for x in batch]\n",
        "    \n",
        "    len_x = torch.LongTensor(len_x)\n",
        "    \n",
        "    batch_x = pad_sequence(batch_x, batch_first=True)\n",
        "    \n",
        "    return batch_x, len_x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4ppavAxbGYv"
      },
      "source": [
        "## 2.3 Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_knptYW7RR73"
      },
      "source": [
        "batch_size = configs['batch_size'] # TODO: decide on batch size\n",
        "\n",
        "# training data\n",
        "train = MyDataSet(train_data, train_labels)\n",
        "train_args = dict(shuffle = True, batch_size = batch_size, num_workers=8, collate_fn=MyDataSet.collate_fn) # TODO: remember to use collate_fn\n",
        "train_loader = DataLoader(train, **train_args)\n",
        "\n",
        "# validation data\n",
        "dev = MyDataSet(dev_data, dev_labels)\n",
        "dev_args = dict(shuffle = False, batch_size = batch_size, num_workers=8, collate_fn=MyDataSet.collate_fn) # TODO: remember to use collate_fn\n",
        "dev_loader = DataLoader(dev, **dev_args)\n",
        "\n",
        "# test data\n",
        "test = TestDataSet(test_data)\n",
        "test_args = dict(shuffle = False, batch_size = batch_size, num_workers=4, collate_fn=TestDataSet.collate_fn) # TODO: remember to use collate_fn\n",
        "test_loader = DataLoader(test, **test_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt_fIhfDbMMm"
      },
      "source": [
        "# 2 Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSSX7kU5XBy_"
      },
      "source": [
        "## 2.1 Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sSvKi9TR_M"
      },
      "source": [
        "# TODO: Create model    \n",
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "    super(LSTMModel, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv1d(in_channels=input_size,out_channels=128,kernel_size=3,padding=1)\n",
        "    self.conv1_bn = nn.BatchNorm1d(128)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.drop_conv = nn.Dropout(0.5)\n",
        "    self.conv2 = nn.Conv1d(in_channels=128,out_channels=256,kernel_size=3,padding=1)\n",
        "    self.conv2_bn = nn.BatchNorm1d(256)\n",
        "    self.lstm = nn.LSTM(input_size=256, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, dropout=0.3)\n",
        "    self.drop1 = nn.Dropout(0.5)\n",
        "    self.fc1 = nn.Linear(hidden_size*2, output_size)\n",
        "    self.pack = nn.utils.rnn.pack_padded_sequence\n",
        "    self.unpack = nn.utils.rnn.pad_packed_sequence\n",
        "\n",
        "    \n",
        "  def forward(self, x, length): \n",
        "\n",
        "    # x: BxLxC\n",
        "    x = x.permute(0, 2, 1)\n",
        "    # x: BxCxL\n",
        "    if self.conv1 is not None:\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv1_bn(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.drop_conv(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv2_bn(x)\n",
        "\n",
        "    # x: BxCxL\n",
        "    x = x.permute(2, 0, 1)\n",
        "    # x: LxBxC\n",
        "    x = self.pack(x, length, batch_first=False, enforce_sorted=False)\n",
        "    x, _ = self.lstm(x)\n",
        "    x, _ = self.unpack(x, batch_first=False)\n",
        "    # x: LxBxC\n",
        "    x = x.permute(1, 0, 2)\n",
        "    # x: BLxC\n",
        "    B, L, C = x.shape\n",
        "    #x = x.flatten(0, 1)\n",
        "    x = self.drop1(x)\n",
        "    x = self.fc1(x)\n",
        "    # x: BxLxC\n",
        "    x = x.view(B, L, 42)\n",
        "    x = x.permute(1, 0, 2)\n",
        "    x = x.log_softmax(2)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_2OrOTObTEy"
      },
      "source": [
        "## 2.2 Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q8xJFgnZT1v",
        "outputId": "45c2ce92-3357-4ef7-a27e-1f2756629cab"
      },
      "source": [
        "# create model\n",
        "input_size = 40\n",
        "hidden_size = 512  # LSTM hidden size\n",
        "num_layers = 4\n",
        "output_size = 42\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMModel(\n",
            "  (conv1): Conv1d(40, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv1_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (tanh): Tanh()\n",
            "  (drop_conv): Dropout(p=0.5, inplace=False)\n",
            "  (conv2): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv2_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lstm): LSTM(256, 512, num_layers=4, dropout=0.3, bidirectional=True)\n",
            "  (drop1): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=1024, out_features=42, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1hBJnqPjxyu"
      },
      "source": [
        "# 4 Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azt5PaudbbmK"
      },
      "source": [
        "## 4.0 Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6cbZIpDBWFa",
        "outputId": "e37b8cb5-0adb-4d66-b820-022d5f41cae4"
      },
      "source": [
        "# Hyperparams\n",
        "configs['lr'] = 1e-5 #, 5e-4\n",
        "configs['weight_decay'] = 2e-5\n",
        "criterion = nn.CTCLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=configs['lr'], weight_decay=configs['weight_decay'])\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=configs['step'], gamma=configs['lr_decay'],verbose=True)\n",
        "\n",
        "# You can add a LR scheduler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-05.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWLIH1eNbjtg"
      },
      "source": [
        "## 4.1 Train Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtUS0ck-aC5-"
      },
      "source": [
        "# Train the model\n",
        "def train_epoch(model, train_loader, criterion, optimizer):\n",
        "  model.train()\n",
        "\n",
        "  avg_loss = 0.0\n",
        "  start = time.time()\n",
        "  loss_list = []\n",
        "  # TODO: Add logic here\n",
        "  for i, (data, label, length, label_length) in enumerate(train_loader):\n",
        "      data = data.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      # clear the gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # compute the model output\n",
        "      out = model(data, length)\n",
        "      loss = criterion(out, label, length, label_length)\n",
        "      loss.backward()\n",
        "      # torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "      optimizer.step()\n",
        "      loss_list.append(loss.cpu().detach().item())\n",
        "      # pbar.update(1)\n",
        "      # iter += 1\n",
        "      if i%100 == 99:\n",
        "          print('Epoch {}: Loss after {} batchs is {}'.format(epoch, i+1,sum(loss_list)/len(loss_list)))\n",
        "\n",
        "      avg_loss += loss.item()\n",
        "\n",
        "  scheduler.step()\n",
        "  torch.cuda.empty_cache()\n",
        "  del data\n",
        "  del length\n",
        "  del label\n",
        "  del label_length\n",
        "  gc.collect()\n",
        "\n",
        "  end = time.time()\n",
        "  avg_loss /= len(train_loader) # average batch loss\n",
        "\n",
        "  print(f'Training loss: {avg_loss} Time: {end - start}')\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk6TiRTS8MZq",
        "outputId": "4c4abc49-650e-4c48-8eb4-517c6ad31432"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idmvOyiEq63H"
      },
      "source": [
        "## 4.2 CTC Decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrIW7OENr7fg"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gdrive/MyDrive/IDL-HW3P2/\")\n",
        "# sys.path.append(\"/content/gdrive/MyDrive/IDL-HW3P2/hw3p2_handout\")\n",
        "sys.path.append('/content/gdrive/MyDrive/IDL-HW3P2/ctcdecode')\n",
        "from phoneme_list import PHONEME_MAP, PHONEME_LIST\n",
        "PHONEME_MAP = np.array(PHONEME_MAP)\n",
        "from Levenshtein import distance as lev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mby2lwGsX-a"
      },
      "source": [
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "PHONEME_MAP = np.array(PHONEME_MAP)\n",
        "# TODO: Initialize decoder here\n",
        "# In CTCBeamDecoder beam_width=1 (greedy search); beam_width>1 (beam search)\n",
        "decoder = CTCBeamDecoder(\n",
        "    PHONEME_MAP,\n",
        "    model_path=None,\n",
        "    alpha=0,\n",
        "    beta=0,\n",
        "    cutoff_top_n=40,\n",
        "    cutoff_prob=1.0,\n",
        "    beam_width=2,\n",
        "    num_processes=8,\n",
        "    blank_id=0,\n",
        "    log_probs_input=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arxNVRFvj0ro"
      },
      "source": [
        "## 4.3 Validate Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_5ydu7ciNye"
      },
      "source": [
        "# Validate the model\n",
        "def validate_model(model, val_loader, criterion):\n",
        "\n",
        "  avg_loss = 0.0\n",
        "  running_dist = 0.0\n",
        "  predictions = []\n",
        "  val_loss = 0\n",
        "  val_dis = 0\n",
        "  count = 0\n",
        "  with torch.no_grad():\n",
        "    # model in validation mode \n",
        "    model.eval()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # TODO: Add logic here (remember to decode output and compute distance)\n",
        "    for batch, (data, label, length, label_length) in enumerate(dev_loader):\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "\n",
        "      data = data.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      count += data.shape[0]\n",
        "      try:\n",
        "        del result\n",
        "        del gt\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "      result = []\n",
        "      gt = []\n",
        "\n",
        "      out = model(data, length)\n",
        "      loss = criterion(out, label, length, label_length)\n",
        "      out = out.permute(1, 0, 2)\n",
        "      beam_results, beam_scores, timesteps, out_lens = decoder.decode(out, length)\n",
        "\n",
        "      del beam_scores\n",
        "      del timesteps\n",
        "      for b in range(beam_results.size()[0]):\n",
        "          result.append(''.join(PHONEME_MAP[beam_results[b][0][:out_lens[b][0]]].tolist()))\n",
        "          g = label.cpu().numpy()[b][:label_length[b]]\n",
        "          gt.append(''.join(PHONEME_MAP[g].tolist()))\n",
        "          \n",
        "      for a, b in zip(result, gt):\n",
        "          val_dis += lev(a, b)\n",
        "\n",
        "      val_loss += loss\n",
        "  \n",
        "  val_loss = val_loss / (count + 1e-5) * configs['batch_size']\n",
        "  val_dis = val_dis / (count + 1e-5)\n",
        "  print('val dist = ', val_dis)\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  del data\n",
        "  del length\n",
        "  del label\n",
        "  del label_length\n",
        "  del gt \n",
        "  del result\n",
        "  del g\n",
        "  gc.collect()\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  print(f'Validation loss: {val_loss} Levenshtein distance: {val_dis} Time: {end - start}')\n",
        "  return val_loss, val_dis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SSwXQw5qLoO"
      },
      "source": [
        "# Test\n",
        "\n",
        "# Train the model\n",
        "def test_model(model, test_loader, criterion):\n",
        "\n",
        "  avg_loss = 0.0\n",
        "  running_dist = 0.0\n",
        "  predictions = []\n",
        "  val_loss = 0\n",
        "  val_dis = 0\n",
        "  count = 0\n",
        "  result = []\n",
        "  with torch.no_grad():\n",
        "    # model in validation mode \n",
        "    model.eval()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # TODO: Add logic here (remember to decode output and compute distance)\n",
        "    for batch, (data, length) in enumerate(test_loader):\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "\n",
        "      data = data.to(device)\n",
        "      count += data.shape[0]\n",
        "\n",
        "      \n",
        "\n",
        "      out = model(data, length)\n",
        "      # loss = criterion(out, label, length, label_length)\n",
        "      out = out.permute(1, 0, 2)\n",
        "      beam_results, beam_scores, timesteps, out_lens = decoder.decode(out, length)\n",
        "\n",
        "      del beam_scores\n",
        "      del timesteps\n",
        "      for b in range(beam_results.size()[0]):\n",
        "          result.append(''.join(PHONEME_MAP[beam_results[b][0][:out_lens[b][0]]].tolist()))\n",
        "  print('result shape = ', len(result))\n",
        "  \n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  end = time.time()\n",
        "  return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pUABCtLSP1F"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8vCpkiblZbY"
      },
      "source": [
        "## 4.4 Run Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjnPWR_MlDNZ",
        "outputId": "8113995c-9919-4cad-b603-eb371128ba7f"
      },
      "source": [
        "# Define number of epochs\n",
        "epochs = configs['max_epoch']\n",
        "\n",
        "# best_loss = float('inf')\n",
        "iter = 0\n",
        "best_acc = 0\n",
        "best_iter = 0\n",
        "best_loss = 5\n",
        "best_epoch = 0\n",
        "best_loss_epoch = 0\n",
        "best_dis = 100\n",
        "\n",
        "print('Start...')\n",
        "for epoch in range(167,epochs):\n",
        "  print('Epoch: ', epoch+1)\n",
        "\n",
        "  training_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "  val_loss, distance = validate_model(model, dev_loader, criterion)\n",
        "\n",
        "  # save the best model\n",
        "  if distance < best_dis:\n",
        "    print('Best loss: {}, epoch: {}'.format(val_loss, epoch + 1))\n",
        "    # TODO: Save model\n",
        "    state = {'net': model.state_dict(), 'optimizer': optimizer.state_dict(), 'scheduler': scheduler.state_dict(), 'epoch': epoch}\n",
        "    torch.save(state, '/content/gdrive/MyDrive/IDL-HW3P2/saved_model/model4__checkpoint.pth'.format(epoch))\n",
        "    best_loss = val_loss\n",
        "    distance = best_dis\n",
        "\n",
        "  print('='*40)\n",
        "print('Done...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start...\n",
            "Epoch:  168\n",
            "Epoch 167: Loss after 100 batchs is 0.20897951304912568\n",
            "Epoch 167: Loss after 200 batchs is 0.20861322298645973\n",
            "Epoch 167: Loss after 300 batchs is 0.20965264052152632\n",
            "Epoch 167: Loss after 400 batchs is 0.21033401224762202\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Training loss: 0.2103012149805551 Time: 1042.5679607391357\n",
            "val dist =  9.889999955045454\n",
            "Validation loss: 0.37384721636772156 Levenshtein distance: 9.889999955045454 Time: 56.98618125915527\n",
            "Best loss: 0.37384721636772156, epoch: 168\n",
            "========================================\n",
            "Epoch:  169\n",
            "Epoch 168: Loss after 100 batchs is 0.20809593454003333\n",
            "Epoch 168: Loss after 200 batchs is 0.20730837017297746\n",
            "Epoch 168: Loss after 300 batchs is 0.2074563957254092\n",
            "Epoch 168: Loss after 400 batchs is 0.20790045350790023\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Training loss: 0.2081608731995572 Time: 1042.4100592136383\n",
            "val dist =  9.895454500475205\n",
            "Validation loss: 0.3717041015625 Levenshtein distance: 9.895454500475205 Time: 56.60877084732056\n",
            "Best loss: 0.3717041015625, epoch: 169\n",
            "========================================\n",
            "Epoch:  170\n",
            "Epoch 169: Loss after 100 batchs is 0.20625919982790947\n",
            "Epoch 169: Loss after 200 batchs is 0.20514985904097557\n",
            "Epoch 169: Loss after 300 batchs is 0.20549147173762322\n",
            "Epoch 169: Loss after 400 batchs is 0.20583646655082702\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Training loss: 0.20602560089184688 Time: 1041.024620771408\n",
            "val dist =  9.903636318619833\n",
            "Validation loss: 0.37694841623306274 Levenshtein distance: 9.903636318619833 Time: 57.073949098587036\n",
            "Best loss: 0.37694841623306274, epoch: 170\n",
            "========================================\n",
            "Epoch:  171\n",
            "Epoch 170: Loss after 100 batchs is 0.20252544313669205\n",
            "Epoch 170: Loss after 200 batchs is 0.20454496785998344\n",
            "Epoch 170: Loss after 300 batchs is 0.20382744813958803\n",
            "Epoch 170: Loss after 400 batchs is 0.20408003974705935\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Training loss: 0.20386485303496266 Time: 1040.5349366664886\n",
            "val dist =  9.886818136878098\n",
            "Validation loss: 0.3753955066204071 Levenshtein distance: 9.886818136878098 Time: 57.065388202667236\n",
            "Best loss: 0.3753955066204071, epoch: 171\n",
            "========================================\n",
            "Epoch:  172\n",
            "Epoch 171: Loss after 100 batchs is 0.2033031179010868\n",
            "Epoch 171: Loss after 200 batchs is 0.20126157879829407\n",
            "Epoch 171: Loss after 300 batchs is 0.20171753813823065\n",
            "Epoch 171: Loss after 400 batchs is 0.20146690495312214\n",
            "Adjusting learning rate of group 0 to 5.0000e-06.\n",
            "Training loss: 0.20188547975414403 Time: 1042.1112287044525\n",
            "val dist =  9.883181773258263\n",
            "Validation loss: 0.3752772808074951 Levenshtein distance: 9.883181773258263 Time: 56.90633845329285\n",
            "Best loss: 0.3752772808074951, epoch: 172\n",
            "========================================\n",
            "Epoch:  173\n",
            "Epoch 172: Loss after 100 batchs is 0.20317218571901322\n",
            "Epoch 172: Loss after 200 batchs is 0.20011969350278377\n",
            "Epoch 172: Loss after 300 batchs is 0.2002212373415629\n",
            "Epoch 172: Loss after 400 batchs is 0.20023655384778977\n",
            "Adjusting learning rate of group 0 to 5.0000e-06.\n",
            "Training loss: 0.20051114575548487 Time: 1041.6453306674957\n",
            "val dist =  9.867272682421486\n",
            "Validation loss: 0.37801915407180786 Levenshtein distance: 9.867272682421486 Time: 56.71208071708679\n",
            "Best loss: 0.37801915407180786, epoch: 173\n",
            "========================================\n",
            "Epoch:  174\n",
            "Epoch 173: Loss after 100 batchs is 0.1944006797671318\n",
            "Epoch 173: Loss after 200 batchs is 0.19777735613286496\n",
            "Epoch 173: Loss after 300 batchs is 0.19845818037788074\n",
            "Epoch 173: Loss after 400 batchs is 0.1986108708009124\n",
            "Adjusting learning rate of group 0 to 5.0000e-06.\n",
            "Training loss: 0.1989279765349168 Time: 1042.1883924007416\n",
            "val dist =  9.799545410002066\n",
            "Validation loss: 0.37644147872924805 Levenshtein distance: 9.799545410002066 Time: 56.8228063583374\n",
            "Best loss: 0.37644147872924805, epoch: 174\n",
            "========================================\n",
            "Epoch:  175\n",
            "Epoch 174: Loss after 100 batchs is 0.19865248516201972\n",
            "Epoch 174: Loss after 200 batchs is 0.19858749970793724\n",
            "Epoch 174: Loss after 300 batchs is 0.198280461281538\n",
            "Epoch 174: Loss after 400 batchs is 0.1979615719243884\n",
            "Adjusting learning rate of group 0 to 5.0000e-06.\n",
            "Training loss: 0.1987888806796336 Time: 1036.929076910019\n",
            "val dist =  9.794999955477271\n",
            "Validation loss: 0.3761052191257477 Levenshtein distance: 9.794999955477271 Time: 56.33582329750061\n",
            "Best loss: 0.3761052191257477, epoch: 175\n",
            "========================================\n",
            "Epoch:  176\n",
            "Epoch 175: Loss after 100 batchs is 0.20076689690351487\n",
            "Epoch 175: Loss after 200 batchs is 0.19867297198623418\n",
            "Epoch 175: Loss after 300 batchs is 0.19850874491035939\n",
            "Epoch 175: Loss after 400 batchs is 0.19743992639705538\n",
            "Adjusting learning rate of group 0 to 5.0000e-06.\n",
            "Training loss: 0.19746938103830422 Time: 1040.949333190918\n",
            "val dist =  9.81681813719628\n",
            "Validation loss: 0.37795549631118774 Levenshtein distance: 9.81681813719628 Time: 56.22087574005127\n",
            "Best loss: 0.37795549631118774, epoch: 176\n",
            "========================================\n",
            "Epoch:  177\n",
            "Epoch 176: Loss after 100 batchs is 0.1978594571352005\n",
            "Epoch 176: Loss after 200 batchs is 0.1968465615808964\n",
            "Epoch 176: Loss after 300 batchs is 0.19696778332193693\n",
            "Epoch 176: Loss after 400 batchs is 0.19715254195034504\n",
            "Adjusting learning rate of group 0 to 2.5000e-06.\n",
            "Training loss: 0.19709258007479238 Time: 1038.5359253883362\n",
            "val dist =  9.813636319028925\n",
            "Validation loss: 0.3777652680873871 Levenshtein distance: 9.813636319028925 Time: 56.52618861198425\n",
            "Best loss: 0.3777652680873871, epoch: 177\n",
            "========================================\n",
            "Epoch:  178\n",
            "Epoch 177: Loss after 100 batchs is 0.19460395529866217\n",
            "Epoch 177: Loss after 200 batchs is 0.19520799450576307\n",
            "Epoch 177: Loss after 300 batchs is 0.19506791586677233\n",
            "Epoch 177: Loss after 400 batchs is 0.1960548746585846\n",
            "Adjusting learning rate of group 0 to 2.5000e-06.\n",
            "Training loss: 0.19593669100777134 Time: 1040.1035454273224\n",
            "val dist =  9.800909046359504\n",
            "Validation loss: 0.3777868151664734 Levenshtein distance: 9.800909046359504 Time: 56.258761405944824\n",
            "Best loss: 0.3777868151664734, epoch: 178\n",
            "========================================\n",
            "Epoch:  179\n",
            "Epoch 178: Loss after 100 batchs is 0.19122500583529473\n",
            "Epoch 178: Loss after 200 batchs is 0.19287043310701846\n",
            "Epoch 178: Loss after 300 batchs is 0.19403212279081344\n",
            "Epoch 178: Loss after 400 batchs is 0.19464964628219605\n",
            "Adjusting learning rate of group 0 to 2.5000e-06.\n",
            "Training loss: 0.1949090667805829 Time: 1040.1986091136932\n",
            "val dist =  9.769999955590908\n",
            "Validation loss: 0.37759163975715637 Levenshtein distance: 9.769999955590908 Time: 56.213027477264404\n",
            "Best loss: 0.37759163975715637, epoch: 179\n",
            "========================================\n",
            "Epoch:  180\n",
            "Epoch 179: Loss after 100 batchs is 0.19027643620967866\n",
            "Epoch 179: Loss after 200 batchs is 0.19182269990444184\n",
            "Epoch 179: Loss after 300 batchs is 0.19419852683941524\n",
            "Epoch 179: Loss after 400 batchs is 0.19375110622495412\n",
            "Adjusting learning rate of group 0 to 2.5000e-06.\n",
            "Training loss: 0.1946293320957121 Time: 1039.1642746925354\n",
            "val dist =  9.78363631916529\n",
            "Validation loss: 0.3779356777667999 Levenshtein distance: 9.78363631916529 Time: 56.45345592498779\n",
            "Best loss: 0.3779356777667999, epoch: 180\n",
            "========================================\n",
            "Epoch:  181\n",
            "Epoch 180: Loss after 100 batchs is 0.19436987787485122\n",
            "Epoch 180: Loss after 200 batchs is 0.19421399593353272\n",
            "Epoch 180: Loss after 300 batchs is 0.19465944096446036\n",
            "Epoch 180: Loss after 400 batchs is 0.19446522288024426\n",
            "Adjusting learning rate of group 0 to 2.5000e-06.\n",
            "Training loss: 0.19436013728052706 Time: 1041.6435194015503\n",
            "val dist =  9.781363591902892\n",
            "Validation loss: 0.37746337056159973 Levenshtein distance: 9.781363591902892 Time: 56.656798124313354\n",
            "Best loss: 0.37746337056159973, epoch: 181\n",
            "========================================\n",
            "Epoch:  182\n",
            "Epoch 181: Loss after 100 batchs is 0.18938083529472352\n",
            "Epoch 181: Loss after 200 batchs is 0.19060136750340462\n",
            "Epoch 181: Loss after 300 batchs is 0.19056346600254376\n",
            "Epoch 181: Loss after 400 batchs is 0.19205689143389462\n",
            "Adjusting learning rate of group 0 to 1.2500e-06.\n",
            "Training loss: 0.19312665190670517 Time: 1041.0734422206879\n",
            "val dist =  9.787727228237603\n",
            "Validation loss: 0.37868696451187134 Levenshtein distance: 9.787727228237603 Time: 56.42127871513367\n",
            "Best loss: 0.37868696451187134, epoch: 182\n",
            "========================================\n",
            "Epoch:  183\n",
            "Epoch 182: Loss after 100 batchs is 0.19379195407032968\n",
            "Epoch 182: Loss after 200 batchs is 0.19486355483531953\n",
            "Epoch 182: Loss after 300 batchs is 0.1940007549027602\n",
            "Epoch 182: Loss after 400 batchs is 0.1932166138663888\n",
            "Adjusting learning rate of group 0 to 1.2500e-06.\n",
            "Training loss: 0.19314316946726579 Time: 1040.3999545574188\n",
            "val dist =  9.805909046336776\n",
            "Validation loss: 0.37937790155410767 Levenshtein distance: 9.805909046336776 Time: 56.46851205825806\n",
            "Best loss: 0.37937790155410767, epoch: 183\n",
            "========================================\n",
            "Epoch:  184\n",
            "Epoch 183: Loss after 100 batchs is 0.19327175080776215\n",
            "Epoch 183: Loss after 200 batchs is 0.19199157796800137\n",
            "Epoch 183: Loss after 300 batchs is 0.19179962227741879\n",
            "Epoch 183: Loss after 400 batchs is 0.19229226060211657\n",
            "Adjusting learning rate of group 0 to 1.2500e-06.\n",
            "Training loss: 0.19266739166699923 Time: 1039.0712759494781\n",
            "val dist =  9.813181773576446\n",
            "Validation loss: 0.37943336367607117 Levenshtein distance: 9.813181773576446 Time: 56.583144426345825\n",
            "Best loss: 0.37943336367607117, epoch: 184\n",
            "========================================\n",
            "Epoch:  185\n",
            "Epoch 184: Loss after 100 batchs is 0.19430952668190002\n",
            "Epoch 184: Loss after 200 batchs is 0.1939829583466053\n",
            "Epoch 184: Loss after 300 batchs is 0.19267435838778812\n",
            "Epoch 184: Loss after 400 batchs is 0.1922298949956894\n",
            "Adjusting learning rate of group 0 to 1.2500e-06.\n",
            "Training loss: 0.19181069875811482 Time: 1040.0551328659058\n",
            "val dist =  9.780909046450413\n",
            "Validation loss: 0.3791808485984802 Levenshtein distance: 9.780909046450413 Time: 56.25826358795166\n",
            "Best loss: 0.3791808485984802, epoch: 185\n",
            "========================================\n",
            "Epoch:  186\n",
            "Epoch 185: Loss after 100 batchs is 0.1909947608411312\n",
            "Epoch 185: Loss after 200 batchs is 0.1921703563630581\n",
            "Epoch 185: Loss after 300 batchs is 0.19156998733679453\n",
            "Epoch 185: Loss after 400 batchs is 0.19186840038746594\n",
            "Adjusting learning rate of group 0 to 1.2500e-06.\n",
            "Training loss: 0.19194309600106962 Time: 1041.941550731659\n",
            "val dist =  9.76863631923347\n",
            "Validation loss: 0.37818941473960876 Levenshtein distance: 9.76863631923347 Time: 56.74482583999634\n",
            "Best loss: 0.37818941473960876, epoch: 186\n",
            "========================================\n",
            "Epoch:  187\n",
            "Epoch 186: Loss after 100 batchs is 0.19391191020607948\n",
            "Epoch 186: Loss after 200 batchs is 0.19315980687737466\n",
            "Epoch 186: Loss after 300 batchs is 0.19211402893066407\n",
            "Epoch 186: Loss after 400 batchs is 0.19214114610105754\n",
            "Adjusting learning rate of group 0 to 6.2500e-07.\n",
            "Training loss: 0.19219579981578575 Time: 1040.673106431961\n",
            "val dist =  9.787727228237603\n",
            "Validation loss: 0.3787286877632141 Levenshtein distance: 9.787727228237603 Time: 56.22747993469238\n",
            "Best loss: 0.3787286877632141, epoch: 187\n",
            "========================================\n",
            "Epoch:  188\n",
            "Epoch 187: Loss after 100 batchs is 0.19668548345565795\n",
            "Epoch 187: Loss after 200 batchs is 0.1940536095201969\n",
            "Epoch 187: Loss after 300 batchs is 0.19306857138872147\n",
            "Epoch 187: Loss after 400 batchs is 0.1924149440228939\n",
            "Adjusting learning rate of group 0 to 6.2500e-07.\n",
            "Training loss: 0.19206513104202982 Time: 1039.0819735527039\n",
            "val dist =  9.787727228237603\n",
            "Validation loss: 0.37859776616096497 Levenshtein distance: 9.787727228237603 Time: 56.68121695518494\n",
            "Best loss: 0.37859776616096497, epoch: 188\n",
            "========================================\n",
            "Epoch:  189\n",
            "Epoch 188: Loss after 100 batchs is 0.19399140834808348\n",
            "Epoch 188: Loss after 200 batchs is 0.19335801266133784\n",
            "Epoch 188: Loss after 300 batchs is 0.1921378096441428\n",
            "Epoch 188: Loss after 400 batchs is 0.1920133351162076\n",
            "Adjusting learning rate of group 0 to 6.2500e-07.\n",
            "Training loss: 0.19140387560640063 Time: 1042.5117735862732\n",
            "val dist =  9.774545410115701\n",
            "Validation loss: 0.3798483908176422 Levenshtein distance: 9.774545410115701 Time: 56.386940240859985\n",
            "Best loss: 0.3798483908176422, epoch: 189\n",
            "========================================\n",
            "Epoch:  190\n",
            "Epoch 189: Loss after 100 batchs is 0.18815908312797547\n",
            "Epoch 189: Loss after 200 batchs is 0.19012464739382268\n",
            "Epoch 189: Loss after 300 batchs is 0.19089996789892513\n",
            "Epoch 189: Loss after 400 batchs is 0.191362618021667\n",
            "Adjusting learning rate of group 0 to 6.2500e-07.\n",
            "Training loss: 0.1913651771270312 Time: 1041.1156690120697\n",
            "val dist =  9.760909046541322\n",
            "Validation loss: 0.3788926899433136 Levenshtein distance: 9.760909046541322 Time: 56.63532853126526\n",
            "Best loss: 0.3788926899433136, epoch: 190\n",
            "========================================\n",
            "Epoch:  191\n",
            "Epoch 190: Loss after 100 batchs is 0.19047340184450148\n",
            "Epoch 190: Loss after 200 batchs is 0.19164096899330615\n",
            "Epoch 190: Loss after 300 batchs is 0.19082423607508342\n",
            "Epoch 190: Loss after 400 batchs is 0.19082790076732636\n",
            "Adjusting learning rate of group 0 to 6.2500e-07.\n",
            "Training loss: 0.19134044650491777 Time: 1041.1576311588287\n",
            "val dist =  9.784999955522727\n",
            "Validation loss: 0.37912631034851074 Levenshtein distance: 9.784999955522727 Time: 56.675052642822266\n",
            "Best loss: 0.37912631034851074, epoch: 191\n",
            "========================================\n",
            "Epoch:  192\n",
            "Epoch 191: Loss after 100 batchs is 0.1928989739716053\n",
            "Epoch 191: Loss after 200 batchs is 0.19157410137355327\n",
            "Epoch 191: Loss after 300 batchs is 0.192287234912316\n",
            "Epoch 191: Loss after 400 batchs is 0.19167891081422567\n",
            "Adjusting learning rate of group 0 to 3.1250e-07.\n",
            "Training loss: 0.19125794981207167 Time: 1044.2150464057922\n",
            "val dist =  9.776818137378099\n",
            "Validation loss: 0.3788456618785858 Levenshtein distance: 9.776818137378099 Time: 57.705963134765625\n",
            "Best loss: 0.3788456618785858, epoch: 192\n",
            "========================================\n",
            "Epoch:  193\n",
            "Epoch 192: Loss after 100 batchs is 0.19070155516266823\n",
            "Epoch 192: Loss after 200 batchs is 0.1898759451508522\n",
            "Epoch 192: Loss after 300 batchs is 0.19007928609848024\n",
            "Epoch 192: Loss after 400 batchs is 0.19088859215378762\n",
            "Adjusting learning rate of group 0 to 3.1250e-07.\n",
            "Training loss: 0.19109497103062306 Time: 1044.4695785045624\n",
            "val dist =  9.77636359192562\n",
            "Validation loss: 0.37877213954925537 Levenshtein distance: 9.77636359192562 Time: 57.22289061546326\n",
            "Best loss: 0.37877213954925537, epoch: 193\n",
            "========================================\n",
            "Epoch:  194\n",
            "Epoch 193: Loss after 100 batchs is 0.19082403302192688\n",
            "Epoch 193: Loss after 200 batchs is 0.1908691196143627\n",
            "Epoch 193: Loss after 300 batchs is 0.19067952677607536\n",
            "Epoch 193: Loss after 400 batchs is 0.19083684526383876\n",
            "Adjusting learning rate of group 0 to 3.1250e-07.\n",
            "Training loss: 0.19075348920874544 Time: 1044.2606279850006\n",
            "val dist =  9.786363591880164\n",
            "Validation loss: 0.37893131375312805 Levenshtein distance: 9.786363591880164 Time: 57.79088807106018\n",
            "Best loss: 0.37893131375312805, epoch: 194\n",
            "========================================\n",
            "Epoch:  195\n",
            "Epoch 194: Loss after 100 batchs is 0.18937610521912573\n",
            "Epoch 194: Loss after 200 batchs is 0.1921631570160389\n",
            "Epoch 194: Loss after 300 batchs is 0.19191536620259286\n",
            "Epoch 194: Loss after 400 batchs is 0.19166674938052894\n",
            "Adjusting learning rate of group 0 to 3.1250e-07.\n",
            "Training loss: 0.19138428149642525 Time: 1042.6526215076447\n",
            "val dist =  9.777272682830578\n",
            "Validation loss: 0.37892085313796997 Levenshtein distance: 9.777272682830578 Time: 57.526731729507446\n",
            "Best loss: 0.37892085313796997, epoch: 195\n",
            "========================================\n",
            "Epoch:  196\n",
            "Epoch 195: Loss after 100 batchs is 0.19453688070178032\n",
            "Epoch 195: Loss after 200 batchs is 0.19236997179687024\n",
            "Epoch 195: Loss after 300 batchs is 0.19280016889174778\n",
            "Epoch 195: Loss after 400 batchs is 0.19172481749206782\n",
            "Adjusting learning rate of group 0 to 3.1250e-07.\n",
            "Training loss: 0.19131807170726442 Time: 1042.3314266204834\n",
            "val dist =  9.77499995556818\n",
            "Validation loss: 0.3791138529777527 Levenshtein distance: 9.77499995556818 Time: 57.38123846054077\n",
            "Best loss: 0.3791138529777527, epoch: 196\n",
            "========================================\n",
            "Epoch:  197\n",
            "Epoch 196: Loss after 100 batchs is 0.19081528782844542\n",
            "Epoch 196: Loss after 200 batchs is 0.18929368712008\n",
            "Epoch 196: Loss after 300 batchs is 0.18904430737098057\n",
            "Epoch 196: Loss after 400 batchs is 0.18999211855232714\n",
            "Adjusting learning rate of group 0 to 1.5625e-07.\n",
            "Training loss: 0.19058686458802485 Time: 1043.5713946819305\n",
            "val dist =  9.787727228237603\n",
            "Validation loss: 0.3785511553287506 Levenshtein distance: 9.787727228237603 Time: 57.37261962890625\n",
            "Best loss: 0.3785511553287506, epoch: 197\n",
            "========================================\n",
            "Epoch:  198\n",
            "Epoch 197: Loss after 100 batchs is 0.1896178613603115\n",
            "Epoch 197: Loss after 200 batchs is 0.1918067456036806\n",
            "Epoch 197: Loss after 300 batchs is 0.1916590601205826\n",
            "Epoch 197: Loss after 400 batchs is 0.1914523923024535\n",
            "Adjusting learning rate of group 0 to 1.5625e-07.\n",
            "Training loss: 0.19166464661503888 Time: 1046.8272325992584\n",
            "val dist =  9.77499995556818\n",
            "Validation loss: 0.3787844479084015 Levenshtein distance: 9.77499995556818 Time: 57.49224281311035\n",
            "Best loss: 0.3787844479084015, epoch: 198\n",
            "========================================\n",
            "Epoch:  199\n",
            "Epoch 198: Loss after 100 batchs is 0.18908923581242562\n",
            "Epoch 198: Loss after 200 batchs is 0.19012157559394838\n",
            "Epoch 198: Loss after 300 batchs is 0.19024049396316212\n",
            "Epoch 198: Loss after 400 batchs is 0.1902776714786887\n",
            "Adjusting learning rate of group 0 to 1.5625e-07.\n",
            "Training loss: 0.19056284329393408 Time: 1041.385606765747\n",
            "val dist =  9.78954541004752\n",
            "Validation loss: 0.3794594705104828 Levenshtein distance: 9.78954541004752 Time: 57.28081655502319\n",
            "Best loss: 0.3794594705104828, epoch: 199\n",
            "========================================\n",
            "Epoch:  200\n",
            "Epoch 199: Loss after 100 batchs is 0.18958499103784562\n",
            "Epoch 199: Loss after 200 batchs is 0.19028971657156946\n",
            "Epoch 199: Loss after 300 batchs is 0.19074089149634044\n",
            "Epoch 199: Loss after 400 batchs is 0.19080963205546142\n",
            "Adjusting learning rate of group 0 to 1.5625e-07.\n",
            "Training loss: 0.19088622010671175 Time: 1044.326464176178\n",
            "val dist =  9.784999955522727\n",
            "Validation loss: 0.3792429268360138 Levenshtein distance: 9.784999955522727 Time: 57.246094703674316\n",
            "Best loss: 0.3792429268360138, epoch: 200\n",
            "========================================\n",
            "Done...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvD2JK1ifND_"
      },
      "source": [
        "# 5 Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1MvZuK2b4TO"
      },
      "source": [
        "## 5.1 Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWKtRY8csvKr"
      },
      "source": [
        "# load model\n",
        "# lr = 9e-7\n",
        "checkpoint = torch.load('/content/gdrive/MyDrive/IDL-HW3P2/saved_model/model4__checkpoint.pth')\n",
        "model.load_state_dict(checkpoint['net'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "epochs = checkpoint['epoch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvA0PKHk54Pg",
        "outputId": "7f66f258-b19d-4656-8757-0f1c53becfc5"
      },
      "source": [
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=configs['step'], gamma=configs['lr_decay'],verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting learning rate of group 0 to 3.1250e-05.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJhffiP66DPu",
        "outputId": "830e237b-dd3e-48ab-b84d-aab46a6af7c0"
      },
      "source": [
        "predictions = test_model(model, test_loader, criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result shape =  2561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I9VwH_ppG3b"
      },
      "source": [
        "## 5.2 Save Predictions to csv File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVyNW-2q9CTB",
        "outputId": "123b61c8-1c42-45bd-a6c6-44b51e329bf4"
      },
      "source": [
        "# Saving predictions to csv file\n",
        "import pandas as pd\n",
        "filename = '/content/gdrive/MyDrive/IDL-HW3P2/submission7-9.75.csv'\n",
        "index = np.arange(0,len(predictions))\n",
        "# index = [str(i)+'.jpg' for i in index]\n",
        "data = {'id':(index),\n",
        "        'label':predictions}\n",
        "pred_df = pd.DataFrame(data)\n",
        "pred_df = pred_df.rename_axis('id',axis=1)\n",
        "print(pred_df)\n",
        "pred_df.to_csv(filename,index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id      id                                              label\n",
            "0        0  .sedDhpRAkthkhlIdiT.ydOnbilIv?UnOhbitmoRhbwtWi...\n",
            "1        1  .DhsekhmpaRtbigAnzHIR.yWhzhTiNkiN.DhfrstpaRtth...\n",
            "2        2  .itWhzkWytRUDhnhveshlWhzhnsyt.aRdlImoRDhnsiksm...\n",
            "3        3  .RUlhplyzbhtUI.ekshlensizhvhvYnhns.tUDhvrcUzWi...\n",
            "4        4    .DispAsijDen.DeRizatDhfAktDhtolmAnrsOldhndisin.\n",
            "...    ...                                                ...\n",
            "2556  2556  .misfrcIldbizestEkaRcYsishmReRcanhkhpshndsolsh...\n",
            "2557  2557  .hndpIskRAbdtUimhRUlhvlyf.DAtprshnHAzhRIlijhn....\n",
            "2558  2558              .DhtitWhznat.inoRdr.mybIenrdonDhjrhl.\n",
            "2559  2559  .WhntsrtWysDhpRhfesrgEvmIhWoRniNjest?rR.bhtHId...\n",
            "2560  2560  .DhponIkeRjizRechdfrTRI.OezfrDAt.ySudHAvTotDhn...\n",
            "\n",
            "[2561 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShgVcbGtiSNQ"
      },
      "source": [
        "## 5.3 Submit Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGd15WZ99tWg",
        "outputId": "1a3145a5-8499-4ddb-bf41-cd9540cd4519"
      },
      "source": [
        "!kaggle competitions submit -c 11785-fall2021-hw3p2 -f '/content/gdrive/MyDrive/IDL-HW3P2/submission7-9.75.csv' -m \"Model4\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 304k/304k [00:03<00:00, 103kB/s]\n",
            "Successfully submitted to 11785 Homework 3 Part 2: Seq to Seq"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTohP4dJ97k8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}